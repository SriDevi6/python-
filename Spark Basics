All the big data products are licensed to Apache Software Foundations( ASF). 
Apache hadoop , Apache Spark, Apache hive , APACHE pig, Apache Sqoop are product of ASF.
Most of the ASF Products are java based. Most of ASF Products are mostly written in scala. So it is preferred to learn spark using scala.

SPARK- Big data processing Framework
Hadoop MP and Spark
  MR:
  AWS def:
  MR follows multiplw steps to run a job
   a challenge to MapReduce is the sequential multi-step process it takes to run a job. With each step, MapReduce reads data from the cluster, performs operations, and writes the results back to HDFS.
   Because each step requires a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.
   Spark was created to address the limitations to MapReduce,
   by doing processing in-memory, reducing the number of steps in a job, and by reusing data across multiple parallel operations. 
   With Spark, only one-step is needed where data is read into memory, operations performed, and the results written backâ€”resulting in a much faster execution.
   Spark also reuses data by using an in-memory cache to greatly speed up machine learning algorithms that repeatedly call a function on the same dataset. 
   Data re-use is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations.
   This dramatically lowers the latency making Spark multiple times faster than MapReduce, especially when doing machine learning, and interactive analytics
  MR uses Disk to perform i/p and o/p operations.
  Assume we have Data of 10GB which is stored in HDFS (ie disk),
  we have to write a map reduce job, the op of this map reduce job is written back to hdfs,
  if we have anothe mr job which would consume this data , 
  then we would have to again read the data from this hdfs and process it write the data to hdfs
  so it is going to be slow.
  Everytime you want to run MR Program , the data has to be loaded from HDFS(Disk) to your memory and get it processed and then again the output is thrown back to the disk that is HDFS. This made MR slow
SP:
  The data will be initially residing in HDFS. When we write a spark job, the data will be loaded into memory(RAM) , 
  data will be processed in memory and the intermediate results is also written back to the memory.
  As the data is residing in memory makes Spark's processing faster compared to MP. so spark is called Distributed IN- MEMORY Framework.
  
  When to use SP or MR?
 Speed:
   If data has to be processed really fast, then solutions are written using Spark.
   If data can be processed in some delay (with low latency),then solutions are written using MR.
  Cost:
   MR uses disk , which is less in cost compared to RAM used by Spark
   eg 2TB-4000INR, 12BG-4000INR
  Based on the priority, Companies adopt either MR or Spark or sometimes both.
    Data has to be processed really quick, then invest lot of money in spark clusters 
    or go with else Map Reduce.
    If you dont require results immediately, Map reduce can be used instead of Spark.
    
 Writing MR program means not writting MR Code, instead we can use hive which INTERNALLY uses MR
 Framework for different big data problems
 Limitation OF mr?
  refer img.
 How Spark is fast?
   in memory processing combined with its DAG based data processing engine makes Spark very efficient.
 Spark replacement of Hadoop?
   Spark is just a processing engine.
   Spark is not meant to process the data not store the data.
 Some companies use both MR Cluster and Saprk Cluster.
   refer img.

 
   
 
 MR- Batch
 Spark- Batch,Real time (Processing the data asap).
 Use case:
 Financial risk estimation:
 Some people will default the loan ,
 so big data will help in indentifying the risk in financial institution.
 
   
 
 
    
    
    
    
  
  
  
